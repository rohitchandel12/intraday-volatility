{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extreme volatility considers extreme prices during a trading day to descrbbe how volatile a stock is. In this notebook, four kinds of extreme volatilities are introduced. The data used here is minute data with high, low, open and close.  \n",
    "Before implementing specific extreme volitility metrics, let's first review how these kinds of volitilities were introduced and what are their advantages over traditional volitility.  \n",
    "\n",
    "In all formulas below, $h_i$, $l_i$, $cl_i$,$o_i$ are high, low, close and open prices for interval i respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parkinson(1980)\n",
    "It is generally accepted that stock price (S) of stocks follows a geometric brownian process. In other words, ln(S)follows a random walk with a constant difussion constant D. With some derivation, we could find D is the variance of the displacement $x-x_{0}$ after a unit time interval, where x is the log return of stock prices. This suggests a traditional way to estimate D: $D_{x}=\\frac{1}{n-1} \\sum_{i=1}^n(d_{i}-\\bar{d})^2$ , where $d_{i}$ is the log return of period i and $\\bar{d}$ is the mean of log return.\n",
    "\n",
    "Then let's consider the extreme volitility here. Let l denote $x_{max}-x_{min}$. From Feller's research in 1951, we could calculate the probality function of $l$ and then derivate $E[l]$ and E[$l^2$]. The estimation for D with extreme value is $D_{l}=\\frac{1}{4ln(2)n} \\sum_{i=1}^n(l_{i})^2$. This estimation's value matches the volatility estimated in traditional ways.\n",
    "\n",
    "The major difference between these two methods is their variance. Let's compute the variance of them.  \n",
    "$E[(D_{x}-D)^2]=[\\frac{E[x^4]}{E[x^2]}-1] \\frac{D^2}{N_{x}}=\\frac{2D^2}{N_{x}}$  \n",
    "$E[(D_{l}-D)^2]=[\\frac{E[l^4]}{E[l^2]}-1] \\frac{D^2}{N_{l}}=\\frac{0.41D^2}{N_{l}}$  \n",
    "Where N is the number of observations.   \n",
    "\n",
    "Thus, to achieve the same amount of variance, we need $N_{x}$ around $5N_{l}$. The estreme value method is superior to the traditional method with less data requirement and lower sensitivity.   \n",
    "\n",
    "According to the this research, our first extreme volitility metric here is Parkinson extreme volitility. It uses information of every minute's high and low. The formula is as below:  \n",
    "Parkinson (1980):  $   \\begin{equation*} \\sigma = \\sqrt{\\frac{1}{4Nln2}\\sum_{i = 1}^N(ln\\frac{h_i}{l_i})^2} \\end{equation*}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garman Klass (1980)\n",
    "\n",
    "In Mark B. Garman and Michael J. Klass's work, they also assumed that stock's log return follows a random walk process. Besides, it assumed that stocks are continuously traded in the time interval, which fits intraday trading's feature pretty well. On this basis, open and close prices are included in the estimation of extreme volitility. For example, at time 0, let $o_{0}, h_{0}, l_{0}, c_{0}$ denote log open, high, low and close price. Because stocks are continuously traded, we have $o_{t+1} = c_{t}$ for each period. Thus, we could present extreme volitility D as a function of h, l, c and correlation among these three variables.  \n",
    "\n",
    "Let's say the extreme volatility is D(h,l,c). D(h,l,c) should inherit the invariance properties of the joint density of (h,l,c): price symmetry, time symmetry and scale-invariance. Thus, D should satisfy the following rules:  \n",
    "$D(h,l,c)=D(-h,-l,-c), D(h,l,c)=D(h-c,l-c,c-c), D(ah,al,ac)=a^2D(h,l,c)$  \n",
    "It can be shown that these conditions imply that D(h,l,c) must be quadratic. Thus we have:  \n",
    "$D(h,l,c)=a_{200} h^2 +a_{020} l^2 +a_{002} c^2 +a_{110} hl+a_{101} hc+a_{011} lc$, where $a_{200}=a_{020}, a_{101}=a_{011}$ and $2a_{200} + a_{110} + 2a_{101} = 0 $  \n",
    "\n",
    "Insisting that D(h,l,c) be unbiased, that is, $E[D(h,l,c)] = σ^2$, leads to further reduction of parameters: $E[h^2] = E[l^2] = E[c^2] = E[c(h+l)] = σ^2$ and $E[hl] = (1 - 2 ln2)σ^2$, we may restrict attention further to the two-parameter family of D(.) of the form:  \n",
    "$D(h,l,c)=a_{1}(h−l)^2 +a_{2}[c(h+l)−2hl]+[1−4(a_{1}+a_{2})ln2+a_{2}]c^2$\n",
    "\n",
    "To have the best estimator, D(h,l,c) should also have the lowest variance. Applying first order condition to this formula, we could derive the best set of parameters is $ a_{1} ^* = 0.511$ and $a_{2} ^* = −0.019$. Thus we have   \n",
    "$σ^2 =0.511(u−d)^2 −0.019{c(h+l)−2hl}−0.383c^2 $\n",
    "\n",
    "To make the estimator more practical, the paper recommended an estimator $σ^2 =0.5(u−d)^2 −(2ln2-1)c^2 $, which possesses nearly the same efficiency but eliminates the cross-product terms. In the end, we have this formula:\n",
    "\n",
    "Garman Klass (1980):  $ \\begin{equation*} \\sigma = \\sqrt{\\frac{1}{N}\\sum_{i = 1}^N\\frac{1}{2}(ln\\frac{h_i}{l_i})^2 - \\frac{1}{N}\\sum_{i = 1}^N(2ln2-1)(ln\\frac{c_i}{c_{i-1}})^2}  \\end{equation*}$  \n",
    "\n",
    "This metric is rather efficient compared to traditional volatility and Parkinson's extreme volitility. However, it also has a hidden problem. Even though the model assumes continuous trade, in reality data is collected diecretely, for example in a minute level. This will lead to conservative high and low price compared to totally continuous case. Thus, this estimator is underbiased if the data frequency is not high enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rogers and Satchell \n",
    "\n",
    "Another drawback of Garman & Klass method is that it assume the log return is a brownian motion without drift term. To refine it, Rogers and Satchell proposed a new extreme volitility in 1991.  \n",
    "\n",
    "To deal with brownian motions with non-zero drifts, a new estimator was defined: $D=S(S-x)+I(I-x)$, where x is the log return $lnc-lno$, $S=sup(x)=lnh-lno$ and $I=inf(x)=lnl-lno$. With exponential distribution's feature and some derivation, it can be proved that this estimator can deal with non-zero drift situations. Even though estimation of this method is not as efficient as Garman and Klass's estimator when the drift is zero, it doesn't lose too much efficiency, which is acceptable. Thus we have Rogers and satchell (1991):  \n",
    "$   \\begin{equation*}  \\sigma = \\sqrt{\\frac{1}{N}\\sum_{i = 1}^N(ln\\frac{h_i}{c_i})(ln\\frac{h_i}{o_i}) + (ln\\frac{l_i}{c_{i}}) (ln\\frac{l_i}{o_{i}})}  \\end{equation*}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-defined Extreme volitility \n",
    "The last intraday volatility is self-defined, which employs trading experience of my own. When observating intraday performance of a stock, people are likely to notice several local high and local low points at first. All these local points together with trends between each continuous local points make waves of a price plot. Between two continuous local extreme points, there are local trends, which describe how volatile a stock is in short term. For stocks with the same highest price and lowest price of a day, the more short-term trends a stock has, the more severe every short-term trend is, the more volatile a stock should be in a day.  \n",
    "According to this observation, I constructed the last metric, which is also similar to metrics described above:  \n",
    "$   \\begin{equation*} \\sigma = \\sqrt{\\sum_{j = 1}^N(ln\\frac{extreme_j}{extreme_{j-1}})^2} \\end{equation*}$    \n",
    "where $extreme_j$ denotes the local extreme values of a stock close price trend during a day.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a class defined to find peak points(local extreme points) \n",
    "class peak(object):\n",
    "#==============================================================================\n",
    "#find peak points of time series\n",
    "#peak points decided when two ma lines meet\n",
    "#MA_length1, MA_length2: window length of two ma   lines\n",
    "#last: last price of 1 period\n",
    "#==============================================================================\n",
    "\n",
    "    def __init__(self,MA_length1,MA_length2):\n",
    "        self.last=np.array([])\n",
    "        self.n1=MA_length1\n",
    "        self.n2=MA_length2\n",
    "        self.n3=max(self.n1,self.n2)\n",
    "        self.MA_1=[]\n",
    "        self.MA_2=[]\n",
    "        self.section_dict=[(0,'None')]\n",
    "        self.diff=[0]*(self.n3-1)\n",
    "        #index of effective peak points\n",
    "        self.min_peak=[]\n",
    "        self.max_peak=[]\n",
    "        #index of all peak points\n",
    "        self.min_peak_all=[]\n",
    "        self.max_peak_all=[]\n",
    "        \n",
    "        self.min_now=[]\n",
    "        self.max_now=[]\n",
    "        self.latest_peak = \"None\"\n",
    "        self.trend_inc=[]\n",
    "        self.trend_dec=[]\n",
    "\n",
    "    def find_peak(self,data):\n",
    "        temp=data\n",
    "        self.last=np.append(self.last,temp)\n",
    "        self.current_wave=(self.last.max()/self.last.min()-1)\n",
    "        \n",
    "        if len(self.last)>=max(self.n1,self.n2):\n",
    "            self.MA_1.append(self.last[-self.n1:].mean())\n",
    "            self.MA_2.append(self.last[-self.n2:].mean())\n",
    "            self.diff.append(self.MA_1[-1]-self.MA_2[-1])\n",
    "        else:\n",
    "            self.MA_1.append(self.last[-1])\n",
    "            self.MA_2.append(self.last[-1])\n",
    "           \n",
    "        if len(self.last) <= self.n3:\n",
    "            self.min_now.append(np.argmin(self.last.min()))\n",
    "            self.max_now.append(np.argmax(self.last.max()))\n",
    "            self.trend_inc.append(0)\n",
    "            self.trend_dec.append(0)\n",
    "\n",
    "        if len(self.last) > self.n3:\n",
    "            if self.diff[-1]==0:\n",
    "                self.diff[-1]=self.diff[-2]\n",
    "            \n",
    "            if self.diff[-1]>0 and self.diff[-1]*self.diff[-2]<0 and self.latest_peak != \"min\":\n",
    "                self.section_dict.append((len(self.diff)-1,'min'))\n",
    "                start=self.section_dict[-2][0]\n",
    "                end=self.section_dict[-1][0]\n",
    "                self.min_peak_all.append(start+np.argmin(self.last[start:end]))                   \n",
    "                last_three = [self.last[self.max_now[-1]],self.last[start:end].min(),self.last[self.min_now[-1]]]\n",
    "                if max(last_three)/min(last_three)<1+self.current_wave/10:\n",
    "                    if len(self.max_peak)>0: del self.max_peak[-1]\n",
    "                    if self.last[self.min_now[-1]]>self.last[start:end].min() and len(self.min_peak)>0:\n",
    "                        self.min_peak[-1] = start+np.argmin(self.last[start:end])\n",
    "                else:\n",
    "                    self.min_peak.append(start+np.argmin(self.last[start:end]))\n",
    "                self.latest_peak = \"min\"                       \n",
    "                if len(self.min_peak)>0: self.min_now.append(self.min_peak[-1])\n",
    "                else: self.min_now.append(self.min_now[-1])\n",
    "                if len(self.max_peak)>0: self.max_now.append(self.max_peak[-1])\n",
    "                else: self.max_now.append(self.max_now[-1])\n",
    "\n",
    "                \n",
    "            elif self.diff[-1]<0 and self.diff[-1]*self.diff[-2]<0 and self.latest_peak != \"max\" :\n",
    "                self.section_dict.append((len(self.diff)-1,'max'))\n",
    "                start=self.section_dict[-2][0]\n",
    "                end=self.section_dict[-1][0]\n",
    "                self.max_peak_all.append(start+np.argmax(self.last[start:end]))\n",
    "                last_three = [self.last[self.max_now[-1]],self.last[start:end].max(),self.last[self.min_now[-1]]]\n",
    "                if max(last_three)/min(last_three)<1.0016:     \n",
    "                    if len(self.min_peak)>0: del self.min_peak[-1]\n",
    "                    if self.last[self.max_now[-1]]<self.last[start:end].max() and len(self.max_peak)>0:\n",
    "                        self.max_peak[-1] = start+np.argmax(self.last[start:end])\n",
    "                else:\n",
    "                    self.max_peak.append(start+np.argmax(self.last[start:end]))\n",
    "                self.latest_peak = \"max\"                       \n",
    "                if len(self.min_peak)>0: self.min_now.append(self.min_peak[-1])\n",
    "                else: self.min_now.append(self.min_now[-1])\n",
    "                if len(self.max_peak)>0: self.max_now.append(self.max_peak[-1])\n",
    "                else: self.max_now.append(self.max_now[-1])\n",
    "                \n",
    "            else:\n",
    "                self.min_now.append(self.min_now[-1])\n",
    "                self.max_now.append(self.max_now[-1])\n",
    "\n",
    "            if len(self.min_peak)>2:             \n",
    "#                self.trend_inc.append((self.last[self.min_peak[-1]]-self.last[self.min_peak[-2]])/(self.last[self.min_peak[-2]])/((self.min_peak[-1]-self.min_peak[-2])))\n",
    "                self.trend_inc.append((self.last[self.min_peak[-1]]-self.last[self.min_peak[-2]])/(self.last[self.min_peak[-2]]))                \n",
    "            else: self.trend_inc.append(0)\n",
    "            if len(self.max_peak)>2:\n",
    "#                self.trend_dec.append((self.last[self.max_peak[-1]]-self.last[self.max_peak[-2]])/(self.last[self.max_peak[-2]])/((self.max_peak[-1]-self.max_peak[-2])))\n",
    "                self.trend_dec.append((self.last[self.max_peak[-1]]-self.last[self.max_peak[-2]])/(self.last[self.max_peak[-2]]))\n",
    "            else:self.trend_dec.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peak(stock_data,ma_1=5,ma_2=10):\n",
    "    peak_data=peak(ma_1,ma_2)\n",
    "    for index in range(len(stock_data)):\n",
    "        minute_data=pd.DataFrame(stock_data.iloc[index]).T\n",
    "        peak_data.find_peak(minute_data.close)\n",
    "    return peak_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see the effect of the local extreme points finding code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp-plot.html'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=os.path.join((os.getcwd())[:-9],'data')\n",
    "trade_list=os.listdir(path)\n",
    "\n",
    "#choose 1 dataset as an example\n",
    "object_stock=trade_list[10]\n",
    "temp_data=pd.read_csv(os.path.join(path,object_stock))\n",
    "temp_data.timestamp=temp_data.timestamp.apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(x)))\n",
    "temp_data.set_index('timestamp',inplace=True)\n",
    "temp_data=temp_data[temp_data.volume!=0]\n",
    "temp_data.dropna(how='any',inplace=True)\n",
    "peak_data=find_peak(temp_data)\n",
    "data=peak_data.last\n",
    "arr_section=np.array(peak_data.section_dict[:])[:,0].astype('int')\n",
    "\n",
    "last_data=go.Scatter(x=list(range(len(data))),y=data,mode='lines',name='last_price')\n",
    "#build up compare data\n",
    "max_point=data.argmax()\n",
    "min_point=data.argmin()\n",
    "compare_index=[0,min(max_point,min_point),max(max_point,min_point),len(data)-1]\n",
    "compare_data=data[compare_index]\n",
    "compare_data=go.Scatter(x=compare_index,y=compare_data,mode='lines',name='simulate_price')\n",
    "peak_min_data=go.Scatter(x=peak_data.min_peak_all,y=data[peak_data.min_peak_all],mode='markers',name='peak_min',marker=dict(size=20,color='green'))\n",
    "peak_max_data=go.Scatter(x=peak_data.max_peak_all,y=data[peak_data.max_peak_all],mode='markers',name='peak_max',marker=dict(size=20,color='red'))\n",
    "#section=go.Scatter(x=arr_section,y=data[arr_section],mode='markers',name='section',marker=dict(size=20,color='black',symbol='cross'))\n",
    "layout=go.Layout(title=object_stock[:-4])\n",
    "#data_plot=[last_data,peak_min_data,peak_max_data,section]\n",
    "data_plot=[last_data,peak_min_data,peak_max_data,compare_data]\n",
    "fig=go.Figure(data=data_plot,layout=layout)\n",
    "plt.plot(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the new window, we can see the minute close price trend of CAT on 23rd, May, 2019. Red points denote local high point the code finds and green points denote local low points. Compared to the simulated price denotes in red line, even though the highest and lowest price of the day are the same, real price is more volatile because of the exsitence of short-term trends, located between two continuous extreme points.  \n",
    "The following part is to calculate four metrics of extreme volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_vol=pd.DataFrame(columns=['Parkinson','Garman_Klass','Rogers_Satchell','local_peak_vol'])\n",
    "for object_stock in trade_list:\n",
    "    temp_data=pd.read_csv(os.path.join(path,object_stock))\n",
    "    temp_data.timestamp=temp_data.timestamp.apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(x)))\n",
    "    temp_data.set_index('timestamp',inplace=True)\n",
    "    temp_data=temp_data[temp_data.volume!=0]\n",
    "    temp_data.dropna(how='any',inplace=True)\n",
    "\n",
    "    #extreme_vol.loc[object_stock[:-7],'traditional']=(np.log(temp_data.close/temp_data.open)).std()\n",
    "    \n",
    "    #parkinson ,Garman_Klass, Rogers_Satchell extreme vol\n",
    "    part1=(np.log(temp_data.high/temp_data.low)**2).sum()\n",
    "    extreme_vol.loc[object_stock[:-7],'Parkinson']=(part1/(4*len(temp_data.high)*np.log(2)))**0.5\n",
    "\n",
    "    part1=(np.log((temp_data.high/temp_data.low).iloc[1:])**2).sum()\n",
    "    part2=(2*np.log(2)-1)*(np.log((temp_data.close/temp_data.close.shift()).iloc[1:])**2).sum()\n",
    "    extreme_vol.loc[object_stock[:-7],'Garman_Klass']=((part1/2-part2)/len(temp_data.high))**0.5\n",
    "\n",
    "    extreme_vol.loc[object_stock[:-7],'Rogers_Satchell']=(((np.log(temp_data.high/temp_data.close)*np.log(temp_data.high/temp_data.open)+np.log(temp_data.low/temp_data.close)*np.log(temp_data.low/temp_data.open)).sum())/len(temp_data.high))**0.5\n",
    "\n",
    "    #find local extreme data\n",
    "    peak_data=find_peak(temp_data)\n",
    "    #combine all the local extreme index\n",
    "    index_all=[0]+peak_data.min_peak+peak_data.max_peak+[len(peak_data.last)-1]\n",
    "    index_all.sort()\n",
    "    extreme_value=peak_data.last[index_all]\n",
    "    extreme_vol.loc[object_stock[:-7],'local_peak_vol']=np.sqrt((np.diff(np.log(list(extreme_value)))**2).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dataframe shows the calculated number of four metrics of different stocks on different date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Parkinson Garman_Klass Rogers_Satchell local_peak_vol\n",
      "TRV_2019-05-28   0.000283762  0.000236233     0.000265545      0.0128039\n",
      "MMM_2019-05-28   0.000356989  0.000331983     0.000343246      0.0128056\n",
      "MMM_2019-05-24   0.000473722  0.000447043     0.000436491      0.0136502\n",
      "MSFT_2019-05-29  0.000541447  0.000520891     0.000558052      0.0174715\n",
      "TRV_2019-05-24   0.000278438  0.000238676     0.000266703       0.010566\n",
      "MCD_2019-05-23   0.000307933  0.000267885     0.000286113      0.0124092\n",
      "VZ_2019-05-29    0.000470771  0.000430031     0.000456101      0.0165103\n",
      "AXP_2019-05-23   0.000382019  0.000340742       0.0003594      0.0146665\n",
      "INTC_2019-05-28  0.000715759  0.000681806     0.000729165       0.033675\n",
      "INTC_2019-05-24  0.000559615  0.000528001     0.000543202      0.0173916\n",
      "CAT_2019-05-23   0.000657662  0.000574821     0.000647577       0.023919\n",
      "PG_2019-05-29    0.000406488  0.000349323     0.000351559      0.0158274\n",
      "WMT_2019-05-24   0.000328471  0.000300579     0.000322604      0.0115512\n",
      "GS_2019-05-24    0.000414599  0.000378927     0.000410762       0.014068\n",
      "GS_2019-05-28    0.000440801   0.00038527     0.000399383      0.0158109\n",
      "WMT_2019-05-28   0.000371046  0.000344502     0.000364516      0.0119647\n",
      "V_2019-05-24     0.000372356  0.000329138     0.000365852       0.010874\n",
      "V_2019-05-28     0.000402052  0.000380815     0.000398683      0.0162604\n",
      "PFE_2019-05-23   0.000487831  0.000478621      0.00049038      0.0151216\n",
      "CSCO_2019-05-29  0.000618578  0.000561799     0.000584582      0.0185098\n",
      "UTX_2019-05-23   0.000757257  0.000653759     0.000694525      0.0309961\n",
      "DWDP_2019-05-29  0.000736624  0.000672925     0.000727339      0.0296189\n",
      "NKE_2019-05-29    0.00068929  0.000588814     0.000672989      0.0262249\n",
      "KO_2019-05-29    0.000439997  0.000378908     0.000440443      0.0145437\n",
      "BA_2019-05-29    0.000617766  0.000566789     0.000595064      0.0201795\n",
      "CVX_2019-05-24   0.000411689  0.000379305     0.000413195      0.0164394\n",
      "JPM_2019-05-29   0.000484297  0.000451119     0.000477531      0.0198867\n",
      "CVX_2019-05-28   0.000392298  0.000343105     0.000377782      0.0132213\n",
      "XOM_2019-05-23   0.000413013  0.000397981      0.00041034      0.0121588\n",
      "IBM_2019-05-23   0.000450437  0.000400315     0.000403795      0.0263926\n",
      "...                      ...          ...             ...            ...\n",
      "JNJ_2019-05-29    0.00092764  0.000842227     0.000840862      0.0485862\n",
      "MMM_2019-05-23   0.000554779  0.000538974     0.000557039      0.0209882\n",
      "TRV_2019-05-23      0.000331  0.000260958     0.000293985       0.014613\n",
      "MCD_2019-05-24   0.000263222  0.000222031     0.000245323      0.0125468\n",
      "AXP_2019-05-24   0.000310094  0.000277117      0.00030756       0.010134\n",
      "AXP_2019-05-28   0.000317798  0.000278751     0.000311879      0.0116785\n",
      "INTC_2019-05-23  0.000749509  0.000725677     0.000771022      0.0287418\n",
      "MCD_2019-05-28   0.000267866  0.000242368     0.000268656      0.0111309\n",
      "CAT_2019-05-24   0.000472825   0.00041956     0.000455878      0.0158675\n",
      "WMT_2019-05-23   0.000494946  0.000517537     0.000642153      0.0145571\n",
      "GS_2019-05-23    0.000537086  0.000507408     0.000517728      0.0241134\n",
      "CAT_2019-05-28   0.000501489  0.000471053      0.00049721      0.0149673\n",
      "UTX_2019-05-28   0.000491047  0.000429193     0.000492986      0.0200692\n",
      "V_2019-05-23     0.000465264  0.000425925     0.000454403      0.0170626\n",
      "PFE_2019-05-28   0.000490676  0.000456519     0.000462631      0.0166717\n",
      "PFE_2019-05-24   0.000380749   0.00037257     0.000387523      0.0129072\n",
      "UTX_2019-05-24   0.000527668  0.000448778     0.000466448      0.0202474\n",
      "WBA_2019-05-29   0.000527174  0.000469996     0.000511677      0.0239891\n",
      "UNH_2019-05-29   0.000733596  0.000633804     0.000689779      0.0241648\n",
      "CVX_2019-05-23    0.00039221  0.000366853     0.000371725      0.0112596\n",
      "XOM_2019-05-28   0.000420404  0.000373081     0.000380813      0.0157347\n",
      "XOM_2019-05-24   0.000351628  0.000339533     0.000350212      0.0140617\n",
      "DIS_2019-05-28   0.000372208  0.000353437     0.000360946      0.0127843\n",
      "AAPL_2019-05-28  0.000681365  0.000626792     0.000649114      0.0189875\n",
      "IBM_2019-05-24   0.000447403  0.000380653     0.000448829      0.0144561\n",
      "HD_2019-05-23    0.000669405  0.000632092     0.000666802      0.0282696\n",
      "MRK_2019-05-23   0.000424276  0.000394148     0.000402898      0.0144172\n",
      "DIS_2019-05-24   0.000335523  0.000319143     0.000327074      0.0105442\n",
      "IBM_2019-05-28   0.000391882  0.000315589     0.000325777      0.0153194\n",
      "AAPL_2019-05-24  0.000520064  0.000510168     0.000523829      0.0172109\n",
      "\n",
      "[120 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(extreme_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parkinson</th>\n",
       "      <th>Garman_Klass</th>\n",
       "      <th>Rogers_Satchell</th>\n",
       "      <th>local_peak_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Parkinson</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985847</td>\n",
       "      <td>0.979172</td>\n",
       "      <td>0.872398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Garman_Klass</th>\n",
       "      <td>0.985847</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988005</td>\n",
       "      <td>0.837739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rogers_Satchell</th>\n",
       "      <td>0.979172</td>\n",
       "      <td>0.988005</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_peak_vol</th>\n",
       "      <td>0.872398</td>\n",
       "      <td>0.837739</td>\n",
       "      <td>0.820681</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Parkinson  Garman_Klass  Rogers_Satchell  local_peak_vol\n",
       "Parkinson         1.000000      0.985847         0.979172        0.872398\n",
       "Garman_Klass      0.985847      1.000000         0.988005        0.837739\n",
       "Rogers_Satchell   0.979172      0.988005         1.000000        0.820681\n",
       "local_peak_vol    0.872398      0.837739         0.820681        1.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extreme_vol.astype(\"float\").corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation of four metrics, we can also find they are highly correlated, which proves that they can catch similarities of intraday volatilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://eranraviv.com/intraday-volatility-measures/  \n",
    "Estimating Variance From High, Low and Closing Prices, L. C. G. Rogers and S. E. Satchell   \n",
    "The Extreme Value Method for Estimating the Variance of the Rate of Return, Michael Parkinson   \n",
    "On the Estimation of Security Price Volatility from Historical Data, Mark B. Garman and Michael J. Klas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
